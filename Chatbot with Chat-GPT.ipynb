{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc591c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "735ac164",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f298f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01b1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fadc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [{'role':'system','content':'Hello, ChatGPT!'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d287705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask GPT: I want to learn Hadoop\n",
      "ChatGPT: That's great! Hadoop is a powerful framework used for processing and analyzing large amounts of data. It provides a scalable and distributed computing environment that is widely used in big data applications. To get started with Hadoop, here are some steps you can follow:\n",
      "\n",
      "1. Understand the basics: Familiarize yourself with the concepts of Hadoop, including its architecture, core components (Hadoop Distributed File System - HDFS, Yet Another Resource Negotiator - YARN), and the MapReduce programming model.\n",
      "\n",
      "2. Set up a Hadoop cluster: You can create a Hadoop cluster on your local machine using tools like Apache Hadoop, Cloudera, or Hortonworks. Alternatively, you can use cloud platforms like Amazon Web Services (AWS) or Google Cloud Platform (GCP) to set up a cluster.\n",
      "\n",
      "3. Learn HDFS: HDFS is the distributed file system of Hadoop. Understand how it works, including its directory structure, data replication, and how to interact with it through commands like hdfs dfs.\n",
      "\n",
      "4. Learn MapReduce: MapReduce is a programming model used to process and analyze large datasets in Hadoop. Learn how to write MapReduce jobs using programming languages like Java or Python. You can find tutorials and examples online to help you practice.\n",
      "\n",
      "5. Explore related technologies: Hadoop has a vast ecosystem of tools and technologies that work alongside it, such as Hive, Pig, Spark, and HBase. Explore these technologies to enhance your understanding and maximize your abilities in big data processing.\n",
      "\n",
      "6. Practice with sample datasets: Find sample datasets, such as the ones provided by Hadoop distributions or public datasets, and practice processing and analyzing them using Hadoop and related technologies.\n",
      "\n",
      "7. Join online communities and forums: Engage with the Hadoop community through online forums like Apache Hadoop mailing lists, StackOverflow, or Reddit. This can help you connect with experienced Hadoop users and learn from their experiences.\n",
      "\n",
      "8. Read books and online resources: There are several books and online resources available that cover Hadoop and its ecosystem in detail. Some recommended books include \"Hadoop: The Definitive Guide\" by Tom White and \"Hadoop in Action\" by Chuck Lam.\n",
      "\n",
      "Remember that learning Hadoop takes time and hands-on practice. Start with the basics, gradually build your knowledge, and don't hesitate to experiment with different tools and technologies. Good luck on your Hadoop learning journey! If you have any specific questions along the way, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = input('Ask GPT: ')\n",
    "    if prompt:\n",
    "        conversation.append({'role':'user','content':prompt})\n",
    "        response = openai.ChatCompletion.create(model=model,messages=conversation)\n",
    "        token_usage = [{'prompt':prompt,'total_tokens_used': response.usage['total_tokens']}]\n",
    "        print(f'ChatGPT: {response.choices[0].message.content}')\n",
    "        conversation.append({'role':'assistant','content':response.choices[0].message.content})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
